{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbde9679-2f70-41e7-ba5c-0b37606df082",
   "metadata": {},
   "source": [
    "# Bayes-by-backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2670500f-fd94-4dc9-986c-ea7deb4ce7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import distrax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "from flax.training.train_state import TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5125b3-aee4-446a-b175-2a6e6c06b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3702474a-8c56-40fa-8b34-1fdace0c6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a19f494-2ae2-4ee5-ae7b-a4adce7c1221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dynamax.utils import datasets\n",
    "from jax.flatten_util import ravel_pytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1bd6a15-5edf-4cd2-8225-573ca762c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(314)\n",
    "train, test = datasets.load_rotated_mnist()\n",
    "\n",
    "X_train, y_train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c40f49-e329-4c5d-92e0-6b1bc1c8b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(1,2))\n",
    "def get_batch_train_ixs(key, num_samples, batch_size):\n",
    "    \"\"\"\n",
    "    Obtain the training indices to be used in an epoch of\n",
    "    mini-batch optimisation.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = num_samples // batch_size\n",
    "    \n",
    "    batch_ixs = jax.random.permutation(key, num_samples)\n",
    "    batch_ixs = batch_ixs[:steps_per_epoch * batch_size]\n",
    "    batch_ixs = batch_ixs.reshape(steps_per_epoch, batch_size)\n",
    "    \n",
    "    return batch_ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40d628b-20db-43c2-9ae9-455d325ef8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VWeights(nn.Module):\n",
    "    dim_out: int = 1\n",
    "    normal_init: Callable = nn.initializers.normal()\n",
    "    \n",
    "    def setup(self):\n",
    "        self.mean = self.param(\"mean\", normal_init, (self.dim_out,))\n",
    "        self.rho = self.param(\"rho\", normal_init, (self.dim_out,))\n",
    "    \n",
    "    def __call__(self, key):\n",
    "        eps = jax.random.normal(key)\n",
    "        w = self.mean + self.rho * eps\n",
    "        \n",
    "        return w\n",
    "        \n",
    "    \n",
    "    def log_prob(self, w):\n",
    "        \"\"\"\n",
    "        Variational log-probability\n",
    "        \n",
    "        TODO: Try logvar trick and compare\n",
    "              to rho-estimate\n",
    "        \"\"\"\n",
    "        mean, rho = self.mean, self.rho\n",
    "        sigma = jnp.log(1 + jnp.exp(rho))\n",
    "        \n",
    "        lprob = distrax.Normal(loc=mean, scale=sigma).log_prob(w)\n",
    "        return lprob\n",
    "    \n",
    "    def sample_and_eval(self, key):\n",
    "        \"\"\"\n",
    "        Sample weights and evaluate its\n",
    "        log-probability\n",
    "        \"\"\"\n",
    "        w = self.__call__(key)\n",
    "        lprob = self.log_prob(w)\n",
    "        \n",
    "        return w, lprob\n",
    "\n",
    "\n",
    "def bbb_lossfn(key, params, apply_fn, X_batch):\n",
    "    w, variational_logprob = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997da4e5-3d25-4a00-a485-9bbc475d4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    dim_out: int\n",
    "    dim_hidden: int = 100\n",
    "    activation: Callable = nn.relu\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.dim_hidden)(x)\n",
    "        x = self.activation(x)\n",
    "        x = nn.Dense(self.dim_hidden)(x)\n",
    "        x = self.activation(x)\n",
    "        x = nn.Dense(self.dim_out)(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "key = jax.random.PRNGKey(314)\n",
    "key_init, key_train = jax.random.split(key)\n",
    "\n",
    "batch = (100, 28 ** 2)\n",
    "model = MLP(1)\n",
    "batch = jnp.ones(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a512f1-0ff0-4e25-b395-9a848cba27eb",
   "metadata": {},
   "source": [
    "### Initialise params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bef22bd4-2984-4bca-94fa-f3ae15f842c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_mean, key_logvar = jax.random.split(key_init)\n",
    "params_mean = model.init(key_mean, batch)\n",
    "\n",
    "num_params, reconstruct_fn = ravel_pytree(params_mean)\n",
    "num_params = len(num_params)\n",
    "\n",
    "params_logvar = jax.random.normal(key_logvar, (num_params,))\n",
    "params_logvar = reconstruct_fn(params_logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de7f34-8127-4297-8db4-7ad115d9206d",
   "metadata": {},
   "source": [
    "### Define sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1fd7a214-40d0-4cee-883a-1a30c61ca0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float, PyTree, Array\n",
    "from chex import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BBBParams:\n",
    "    mean: PyTree[Float]\n",
    "    logvar: PyTree[Float]\n",
    "\n",
    "# Bayes-by-backprop params\n",
    "b3p = BBBParams(\n",
    "    mean=params_mean,\n",
    "    logvar=params_logvar,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a7c17651-c1b6-4be8-9914-3d2ac4cb88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(eps, mean, logvar):\n",
    "    std = jnp.exp(logvar / 2)\n",
    "    weight = mean + std * eps\n",
    "    return weight\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"num_params\", \"reconstruct_fn\"))\n",
    "def sample_params(key, state:BBBParams, num_params, reconstruct_fn:Callable):\n",
    "    eps = jax.random.normal(key, (num_params,))\n",
    "    eps = reconstruct_fn(eps)\n",
    "    params = jax.tree_map(transform, eps, state.mean, state.logvar)\n",
    "    return params\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"num_params\", \"reconstruct_fn\"))\n",
    "def sample_and_eval(key, state, X, num_params, reconstruct_fn):\n",
    "    params_sample = sample_params(key, state, num_params, reconstruct_fn)\n",
    "    return model.apply(params_sample, X).sum()\n",
    "\n",
    "@jax.jit\n",
    "def get_leaves(params):\n",
    "    flat_params, _ = ravel_pytree(params)\n",
    "    return flat_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f3533008-9d3f-43b8-9073-e78f3ccdab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fn(\n",
    "    key: jax.random.PRNGKey,\n",
    "    state: BBBParams,\n",
    "    X: Float[Array, \"num_obs dim_obs\"],\n",
    "    y: Float[Array, \"num_obs\"],\n",
    "    reconstruct_fn: Callable,\n",
    "    scale_obs=1.0,\n",
    "    scale_prior=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    Add more general way to compute observation-model log-probability\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampled params\n",
    "    params = sample_params(key, state, num_params, reconstruct_fn)\n",
    "    params_flat = get_leaves(params)\n",
    "    \n",
    "    # Prior log probability (use initialised vals for mean?)\n",
    "    logp_prior = distrax.Normal(loc=0.0, scale=scale_prior).log_prob(params_flat).sum()\n",
    "    # Observation log-probability\n",
    "    mu_obs = model.apply(params, X).ravel()\n",
    "    logp_obs = distrax.Normal(loc=mu_obs, scale=scale_obs).log_prob(y).sum()\n",
    "    # Variational log-probability\n",
    "    logp_variational = jax.tree_map(\n",
    "        lambda mean, logvar, x: distrax.Normal(loc=mean, scale=jnp.exp(logvar / 2)).log_prob(x),\n",
    "        b3p.mean, b3p.logvar, params_sample\n",
    "    )\n",
    "    logp_variational = get_leaves(logp_variational).sum()\n",
    "    \n",
    "    return logp_variational - logp_prior - logp_obs\n",
    "\n",
    "\n",
    "def lossfn(key, state, X, y, num_samples=10):\n",
    "    # TODO: add costfn as input\n",
    "    keys = jax.random.split(key, num_samples)\n",
    "    cost_vmap = jax.vmap(cost_fn, in_axes=(0, None, None, None, None))\n",
    "    loss = cost_vmap(keys, state, X, y, reconstruct_fn).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fcf74e7c-eb63-43de-86a7-24d08da26d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(60080128., dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfn(key, b3p, X_train[:100], y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "06b8c87c-da26-4945-9f95-47a133e0e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.adam(1e-3)\n",
    "opt_state = tx.init(b3p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1fec3e76-f0ef-45e3-acd2-dfc649e042a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=b3p,\n",
    "    tx=tx\n",
    ")\n",
    "\n",
    "grads = jax.grad(lossfn, 1)(key, b3p, X_train[:100], y_train[:100])\n",
    "opt_state = opt_state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ad54ca4c-b673-45fa-9720-84a61e2d330e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(56161204., dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfn(key, opt_state.params, X_train[:100], y_train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b98081-202b-47bc-889d-8949beb0e85d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [1] Laurent Valentin Jospin, Wray Buntine, Farid Boussaid, Hamid Laga, Mohammed Bennamoun: “Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users”, 2020, IEEE Computational Intelligence Magazine ( Volume: 17, Issue: 2, May 2022); [arXiv:2007.06823](http://arxiv.org/abs/2007.06823)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
