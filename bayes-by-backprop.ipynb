{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbde9679-2f70-41e7-ba5c-0b37606df082",
   "metadata": {},
   "source": [
    "# Bayes-by-backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2670500f-fd94-4dc9-986c-ea7deb4ce7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import distrax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "from flax.training.train_state import TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5125b3-aee4-446a-b175-2a6e6c06b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a19f494-2ae2-4ee5-ae7b-a4adce7c1221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dynamax.utils import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1bd6a15-5edf-4cd2-8225-573ca762c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(314)\n",
    "train, test = datasets.load_rotated_mnist()\n",
    "\n",
    "X_train, y_train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3702474a-8c56-40fa-8b34-1fdace0c6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "85c40f49-e329-4c5d-92e0-6b1bc1c8b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(1,2))\n",
    "def get_batch_train_ixs(key, num_samples, batch_size):\n",
    "    \"\"\"\n",
    "    Obtain the training indices to be used in an epoch of\n",
    "    mini-batch optimisation.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = num_samples // batch_size\n",
    "    \n",
    "    batch_ixs = jax.random.permutation(key, num_samples)\n",
    "    batch_ixs = batch_ixs[:steps_per_epoch * batch_size]\n",
    "    batch_ixs = batch_ixs.reshape(steps_per_epoch, batch_size)\n",
    "    \n",
    "    return batch_ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e40d628b-20db-43c2-9ae9-455d325ef8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VWeights(nn.Module):\n",
    "    dim_out: int = 1\n",
    "    normal_init: Callable = nn.initializers.normal()\n",
    "    \n",
    "    def setup(self):\n",
    "        self.mean = self.param(\"mean\", normal_init, (self.dim_out,))\n",
    "        self.rho = self.param(\"rho\", normal_init, (self.dim_out,))\n",
    "    \n",
    "    def __call__(self, key):\n",
    "        eps = jax.random.normal(key)\n",
    "        w = self.mean + self.rho * eps\n",
    "        \n",
    "        return w\n",
    "        \n",
    "    \n",
    "    def log_prob(self, w):\n",
    "        \"\"\"\n",
    "        Variational log-probability\n",
    "        \n",
    "        TODO: Try logvar trick and compare\n",
    "              to rho-estimate\n",
    "        \"\"\"\n",
    "        mean, rho = self.mean, self.rho\n",
    "        sigma = jnp.log(1 + jnp.exp(rho))\n",
    "        \n",
    "        lprob = distrax.Normal(loc=mean, scale=sigma).log_prob(w)\n",
    "        return lprob\n",
    "    \n",
    "    def sample_and_eval(self, key):\n",
    "        \"\"\"\n",
    "        Sample weights and evaluate its\n",
    "        log-probability\n",
    "        \"\"\"\n",
    "        w = self.__call__(key)\n",
    "        lprob = self.log_prob(w)\n",
    "        \n",
    "        return w, lprob\n",
    "\n",
    "\n",
    "def bbb_lossfn(key, params, apply_fn, X_batch):\n",
    "    w, variational_logprob = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "997da4e5-3d25-4a00-a485-9bbc475d4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    dim_out: int\n",
    "    dim_hidden: int = 100\n",
    "    activation: Callable = nn.relu\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.dim_hidden)(x)\n",
    "        x = self.activation(x)\n",
    "        x = nn.Dense(self.dim_hidden)(x)\n",
    "        x = self.activation(x)\n",
    "        x = nn.Dense(self.dim_out)(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "key = jax.random.PRNGKey(314)\n",
    "key_init, key_train = jax.random.split(key)\n",
    "\n",
    "batch = (100, 28 ** 2)\n",
    "model = MLP(1)\n",
    "batch = jnp.ones(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a512f1-0ff0-4e25-b395-9a848cba27eb",
   "metadata": {},
   "source": [
    "### Initialise params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bef22bd4-2984-4bca-94fa-f3ae15f842c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_mean, key_logvar = jax.random.split(key_init)\n",
    "params_mean = model.init(key_mean, batch)\n",
    "\n",
    "num_params, reconstruct_fn = ravel_pytree(params_mean)\n",
    "num_params = len(num_params)\n",
    "\n",
    "params_logvar = jax.random.normal(key_logvar, (num_params,))\n",
    "params_logvar = reconstruct_fn(params_logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de7f34-8127-4297-8db4-7ad115d9206d",
   "metadata": {},
   "source": [
    "### Define sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1fd7a214-40d0-4cee-883a-1a30c61ca0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float, PyTree\n",
    "from chex import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BBBParams:\n",
    "    mean: PyTree[Float]\n",
    "    logvar: PyTree[Float]\n",
    "    \n",
    "bbbp = BBBParams(\n",
    "    mean=params_mean,\n",
    "    logvar=params_logvar,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d4419e57-c284-43a2-9b69-aeeb91b1860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.flatten_util import ravel_pytree\n",
    "\n",
    "def transform(eps, mean, logvar):\n",
    "    std = jnp.exp(logvar / 2)\n",
    "    weight = mean + std * eps\n",
    "    return weight\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"num_params\", \"reconstruct_fn\"))\n",
    "def sample_params(key, state:BBBParams, num_params, reconstruct_fn:Callable):\n",
    "    eps = jax.random.normal(key, (num_params,))\n",
    "    eps = reconstruct_fn(eps)\n",
    "    params = jax.tree_map(transform, eps, state.mean, state.logvar)\n",
    "    return params\n",
    "\n",
    "\n",
    "def sample_and_eval(key, state, X, num_params, reconstruct_fn):\n",
    "    params_sample = sample_params(key, state, num_params, reconstruct_fn)\n",
    "    return model.apply(params_sample, X).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7fc485b8-90d6-4151-b444-3c4b0cf711ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = sample_and_eval(key, bbbp, X_train[:100], num_params, reconstruct_fn).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "130010ac-d60d-40a2-a6f9-a5eb13460e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_eval = jax.value_and_grad(sample_and_eval, 1)\n",
    "yhat, grads = grad_eval(key, bbbp, X_train[:100], num_params, reconstruct_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9a4541c6-ec32-4eb1-9e6b-df2f41a4ff9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BBBParams(mean=FrozenDict({\n",
       "    params: {\n",
       "        Dense_0: {\n",
       "            bias: (100,),\n",
       "            kernel: (784, 100),\n",
       "        },\n",
       "        Dense_1: {\n",
       "            bias: (100,),\n",
       "            kernel: (100, 100),\n",
       "        },\n",
       "        Dense_2: {\n",
       "            bias: (1,),\n",
       "            kernel: (100, 1),\n",
       "        },\n",
       "    },\n",
       "}), logvar=FrozenDict({\n",
       "    params: {\n",
       "        Dense_0: {\n",
       "            bias: (100,),\n",
       "            kernel: (784, 100),\n",
       "        },\n",
       "        Dense_1: {\n",
       "            bias: (100,),\n",
       "            kernel: (100, 100),\n",
       "        },\n",
       "        Dense_2: {\n",
       "            bias: (1,),\n",
       "            kernel: (100, 1),\n",
       "        },\n",
       "    },\n",
       "}))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(jnp.shape, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7c951dbf-d6f8-43ef-9103-4e1b594bd805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfn(key):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896c555-683f-453b-ac54-7053f389ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo_bern(key, params, apply_fn, X_batch):\n",
    "    \"\"\"\n",
    "    Importance-weighted marginal log-likelihood for\n",
    "    a Bernoulli decoder and Gaussian encoder\n",
    "    \"\"\"\n",
    "    batch_size = len(X_batch)\n",
    "\n",
    "    encode_decode = apply_fn(params, X_batch, key)\n",
    "    w, (mean_w, logvar_w), logit_mean_x = encode_decode\n",
    "    _, num_is_samples, dim_latent = z.shape\n",
    "\n",
    "    std_w = jnp.exp(logvar_w / 2)\n",
    "    var_w = jnp.exp(logvar_w)\n",
    "    \n",
    "    dist_prior = distrax.MultivariateNormalDiag(jnp.zeros(dim_latent),\n",
    "                                                jnp.ones(dim_latent))\n",
    "    dist_decoder = distrax.Bernoulli(logits=logit_mean_x)\n",
    "    dist_posterior = distrax.Normal(mean_w, std_w)\n",
    "\n",
    "    log_prob_w_prior = dist_prior.log_prob(w)\n",
    "    log_prob_x = dist_decoder.log_prob(X_batch).sum(axis=-1)\n",
    "    # Posterior probability\n",
    "    log_prob_w_post = dist_posterior.log_prob(w).sum(axis=-1)\n",
    "    \n",
    "    elbo = log_prob_w_post - log_prob_x + log_prob_w_prior\n",
    "    return -elbo.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b98081-202b-47bc-889d-8949beb0e85d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [1] Laurent Valentin Jospin, Wray Buntine, Farid Boussaid, Hamid Laga, Mohammed Bennamoun: “Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users”, 2020, IEEE Computational Intelligence Magazine ( Volume: 17, Issue: 2, May 2022); [arXiv:2007.06823](http://arxiv.org/abs/2007.06823)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
